{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project4_student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "exdH4Th8t7In",
        "kE0gKt2Jw8xs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasaad/3630/blob/main/project4_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exdH4Th8t7In"
      },
      "source": [
        "# Project 4: Monte Carlo Localization\n",
        "\n",
        "Due Date: Friday, March 26, 2021 @ 11:59 P.M.\n",
        "\n",
        "Student Name: **Enter Name Here**\n",
        "\n",
        "In this project, you will implement the particle filter and use it to improve the execution performance of a simulation robot tracking a path generated by RRT. Additionally, you will see how particle filters can help with the kidnapped robot problem. This is an individual assignment, and you will be held by the Georgia Tech honor code to finish it by yourself. Collaboration at the white board level is allowed with your teammates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4cxyZvrfonF"
      },
      "source": [
        "# Setup\n",
        "\n",
        "**FIRST, visit Files on Canvas, download \"project4_assets.zip\" and upload it to the files tab (in the left bar). You will receive errors if you skip this step**\n",
        "\n",
        "In this section, we'll install pip packages, import python modules, and download the required assets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkYLDAi3vm7z"
      },
      "source": [
        "# Delete this cell before exporting this notebook as a python file for submission\n",
        "!pip install -q gdown plotly pybullet pathlib tqdm\n",
        "\n",
        "!unzip -qo project4_assets.zip\n",
        "!rm -rf duckie_msgs/ geometry_msgs/ nav_msgs/ std_msgs/ rclpy/\n",
        "!mv project4_assets/mock_msgs/* ./\n",
        "!mv project4_assets/urdf ./\n",
        "!mv project4_assets/example_apartment.json ./\n",
        "!mv project4_assets/helpers.py ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECVNY0YdsD4F"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import rclpy\n",
        "from rclpy.node import Node\n",
        "from nav_msgs.srv import GetPlan\n",
        "from nav_msgs.msg import Path\n",
        "from geometry_msgs.msg import Point, Pose, PoseStamped\n",
        "from duckie_msgs.msg import Wheels, Obstacle, ObstacleList, RangeBearingLandmark, RangeBearingLandmarkList\n",
        "\n",
        "from helpers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqOMSHi7_uvq"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "You may use these functions within your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VyY8IG01M3F"
      },
      "source": [
        "def compute_relative(pose_i, pose_j):\n",
        "    \"\"\"Computes the transform T_i^j (from pose_i to pose_j)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pose_i: tuple(x, y, theta)\n",
        "        a tuple containing (x, y, theta) values in world frame\n",
        "    pose_j: tuple(x, y, theta)\n",
        "        a tuple containing (x, y, theta) values in world frame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple(dx, dy, dtheta)\n",
        "        the calculated transform T_i^j\n",
        "    \"\"\"\n",
        "    x_i, y_i, theta_i = pose_i\n",
        "    x_j, y_j, theta_j = pose_j\n",
        "    delta_x_w = x_j - x_i\n",
        "    delta_y_w = y_j - y_i\n",
        "    delta_theta = theta_j - theta_i\n",
        "\n",
        "    iRw = np.array([[np.cos(theta_i), np.sin(theta_i)], \n",
        "                        [-np.sin(theta_i), np.cos(theta_i)]])\n",
        "    delta_xy_w = np.array([delta_x_w, delta_y_w])\n",
        "    delta_xy_i = np.matmul(iRw, delta_xy_w)\n",
        "\n",
        "    delta_x_i = delta_xy_i[0]\n",
        "    delta_y_i = delta_xy_i[1]\n",
        "\n",
        "    return (delta_x_i, delta_y_i, delta_theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yk14DrQQisU"
      },
      "source": [
        "def pose_compose(T_i, T_j):\n",
        "    \"\"\"Computes the product of two transforms (T_i * T_j)\n",
        "​\n",
        "    Parameters\n",
        "    ----------\n",
        "    T_i: tuple(x, y, theta)\n",
        "        a tuple containing (x, y, theta) values for the transform\n",
        "    T_j: tuple(x, y, theta)\n",
        "        a tuple containing (x, y, theta) values for the transform\n",
        "​\n",
        "    Returns\n",
        "    -------\n",
        "    tuple(dx, dy, dtheta)\n",
        "        the calculated transform T_i*T_j\n",
        "    \"\"\"\n",
        "    x_i, y_i, theta_i = T_i\n",
        "    x_j, y_j, theta_j = T_j\n",
        "    result_theta = theta_j + theta_i\n",
        "    R_i = np.array([[np.cos(theta_i), -np.sin(theta_i)], \n",
        "                    [np.sin(theta_i), np.cos(theta_i)]])\n",
        "    xy_j = np.array([x_j, y_j])\n",
        "    xy_result = np.matmul(R_i, xy_j)\n",
        "    result_x = x_i + xy_result[0]\n",
        "    result_y = y_i + xy_result[1]\n",
        "    return (result_x, result_y, result_theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AnEuKH5f0KU"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "In this section, we'll take a look at the simulator/visualizer. The `Simulator` class manages the physics simulation and visualization of the differential drive robot. It also provides methods for interacting with the environment that you will use in your implementation.\n",
        "\n",
        "Below, the cell creates a `sim` object that reads information about the map from the \"example_apartment.json\" file. You may look at the Files tab (in the left bar) to upload and use your own map files. The visualized map shows obstacles in black, landmarks in yellow, and open space in white. You may pan and zoom using the menu bar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDnucrJIf5Y7"
      },
      "source": [
        "sim = Simulator('example_apartment.json')\n",
        "sim.visualize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFBA3oOqDJwO"
      },
      "source": [
        "We can command the robot via the `sim.step()` function. It takes in a duckie_msgs/Wheels.msg object which gives the robot left and right wheel commands.\n",
        "\n",
        "Below, we command the robot to move forward for 10 seconds. You may press the play button to see the simulation. You may also scrub through the timeline manually. Note that pressing the play button multiple times will not pause the playback.\n",
        "\n",
        "You can adjust the video playback speed with the `duration` parameter on the `sim.visualize()` method. Because it is set to 0.1 seconds here, each simulated timestep (1 second here) is visualized in 0.1 seconds in the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3EAxZmGDEkG"
      },
      "source": [
        "sim = Simulator('example_apartment.json', load_landmarks=False)\n",
        "\n",
        "# simulate out 10 seconds total (10 iteration, 1sec per iteration)\n",
        "for _ in range(10):\n",
        "    sim.step(Wheels(left_wheel=3.0, right_wheel=3.0), None, duration=1.0)\n",
        "\n",
        "sim.visualize(duration=0.1) # visualize all 10 simulated seconds in 1 second"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjHJMZP8r5mi"
      },
      "source": [
        "You may experiment with\n",
        "\n",
        "- the robot's initial pose in the simulation (`initial_pose` parameter)\n",
        "- the noise on the odometry/measurements (`odom_sigma_*` and `marker_sigma_*` parameters)\n",
        "- whether the landmarks are loaded in (`load_landmarks` parameter)\n",
        "- whether the grid lines are shown (`show_grid_lines` parameter)\n",
        "\n",
        "Below, a cell demonstrates how to change these parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqxnkksqsW4L"
      },
      "source": [
        "sim = Simulator('example_apartment.json', load_landmarks=True,\n",
        "                odom_sigma_x=1.0, odom_sigma_y=1.0, odom_sigma_h=1.0, \n",
        "                marker_sigma_range=1.0, marker_sigma_bearing=1.0,\n",
        "                initial_pose=[2.0, 3.0, -np.pi/2])\n",
        "\n",
        "with tqdm(total=40) as pbar:\n",
        "    for _ in range(40):\n",
        "        sim.step(Wheels(left_wheel=3.0, right_wheel=3.0), None, duration=1.0)\n",
        "        pbar.update()\n",
        "\n",
        "sim.visualize(show_grid_lines=True, duration=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqgMgnvJvkA8"
      },
      "source": [
        "Additionally, the simulator has utility functions that you will need for your implementation for this assignment.\n",
        "\n",
        "- `sim.get_obstacles()` - returns a list of obstacles in the map\n",
        "- `sim.is_free(x, y)` - boolean whether a circle centered at (x, y) with radius 0.15m intersects with any obstacles/walls\n",
        "- `sim.visible_markers_gt(x, y, theta)` - given a pose, this method will return a list of landmarks visible to a robot at this position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ZP1z9uwyvr"
      },
      "source": [
        "print(\"Obstacles: \", [(ob.x, ob.y, ob.width, ob.height) for ob in sim.get_obstacles().obs])\n",
        "print(\"Free at (-1, -1) Expected False: \", sim.is_free(-1, -1))\n",
        "print(\"Free at (1, 2) Expected False: \", sim.is_free(1, 2))\n",
        "print(\"Free at (1, 1) Expected True: \", sim.is_free(1, 1))\n",
        "print(\"Markers Visible at (3, 1, PI/2): \", [rb.id for rb in sim.visible_markers_gt([3, 1, np.pi / 2])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTUDwdoxRNyd"
      },
      "source": [
        "# Planning\n",
        "\n",
        "You will not need to implement any code in this section. This section recaps the findings from the RRT project. In the cell below, a hardcoded version of the `RRTPlanner` service provided a path from (1, 1) to (6.2, 3.0) with 5 waypoints total. You may directly copy and paste your solution for RRT project into this cell to have your robot be able to plan for any valid start and goal points in the map. You may copy the contents of the `rrt_planner.py` class verbatim into the cell below.\n",
        "\n",
        "To be clear, we are not running ROS2 code in Google Colab, but rather we have mocked out the ROS2 rclpy library and message modules. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdb8luKeWnAP"
      },
      "source": [
        "# You may replace this hardcoded RRTPlanner with your RRT implementation from Project 3\n",
        "\n",
        "class RRTPlanner():\n",
        "\n",
        "    def plan_callback(self, request, response):\n",
        "        response.plan.poses = [\n",
        "            PoseStamped(pose=Pose(position=Point(x=1.0, y=1.0))),\n",
        "            PoseStamped(pose=Pose(position=Point(x=4.0, y=0.8))),\n",
        "            PoseStamped(pose=Pose(position=Point(x=5.0, y=1.2))),\n",
        "            PoseStamped(pose=Pose(position=Point(x=5.2, y=3.2))),\n",
        "            PoseStamped(pose=Pose(position=Point(x=6.2, y=3.0))),\n",
        "        ]\n",
        "        return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMVUoZYTu_Gw"
      },
      "source": [
        "We form a requst using the `GetPlan.Request()` initializer. We give it start and goal points in the map, and a tolerance within which the RRT algorithm can find a goal state.\n",
        "\n",
        "In the cell below, we provide the RRTPlanner `rrt_planner` with obstacles from `sim.get_obstacles()`. Then we trigger the planning callback and visualize the plan using `sim.visualize(plan=result)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgRuoRKRRhyb"
      },
      "source": [
        "# form request\n",
        "request = GetPlan.Request()\n",
        "start_point = Point(x=1.0, y=1.0)\n",
        "goal_point = Point(x=6.2, y=3.0)\n",
        "request.start = PoseStamped(pose=Pose(position=start_point))\n",
        "request.goal = PoseStamped(pose=Pose(position=goal_point))\n",
        "request.tolerance = 0.1 # meters\n",
        "\n",
        "# load obstacles and trigger planning\n",
        "response = GetPlan.Response()\n",
        "sim = Simulator('example_apartment.json', load_landmarks=False)\n",
        "rrt_planner = RRTPlanner()\n",
        "rrt_planner.obstacles = sim.get_obstacles()\n",
        "response = rrt_planner.plan_callback(request, response)\n",
        "\n",
        "# visualize plan\n",
        "sim.visualize(plan=response.plan, show_grid_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbnto92n1aLy"
      },
      "source": [
        "In the following cell, we provide an entirely implemented NaiveController. The controller has a function called `execute_plan()` that is called once per iteration. It is provided a relative odometry update (provided by `sim.step()`). The approach to path tracking implemented in NaiveController is often called Dead Reckoning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vsy7xxK2tNv"
      },
      "source": [
        "class NaiveController:\n",
        "\n",
        "    def __init__(self, plan, initial_pose=[0.0, 0.0, 0.0], max_rotational_speed=1.0, max_translation_speed=2.5):\n",
        "        self.plan = plan\n",
        "        self.goal_waypoint_idx = 0\n",
        "        self.rotation_speed = max_rotational_speed\n",
        "        self.translation_speed = max_translation_speed\n",
        "        self.pose = initial_pose\n",
        "\n",
        "    def _within_tolerance_of_point(self, x, y, waypoint, tolerance_m):\n",
        "        def distance(x1, y1, x2, y2):\n",
        "            return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
        "\n",
        "        return distance(waypoint.x, waypoint.y, x, y) <= tolerance_m\n",
        "\n",
        "    def execute_plan(self, rel_pose):\n",
        "        # integrate rel_pose\n",
        "        self.pose = pose_compose(self.pose, rel_pose)\n",
        "\n",
        "        # check if arrived to waypoint\n",
        "        if self.goal_waypoint_idx < len(self.plan.poses) and \\\n",
        "           self._within_tolerance_of_point(self.pose[0], self.pose[1], self.plan.poses[self.goal_waypoint_idx].pose.position, tolerance_m=0.05):\n",
        "            self.goal_waypoint_idx += 1\n",
        "\n",
        "        # check if execution complete\n",
        "        if self.goal_waypoint_idx >= len(self.plan.poses):\n",
        "            return Wheels() # stop command\n",
        "\n",
        "        # perform rotation if required\n",
        "        robot_frame = np.array([[np.cos(self.pose[2]), - np.sin(self.pose[2]), self.pose[0]],\n",
        "                                [np.sin(self.pose[2]),   np.cos(self.pose[2]), self.pose[1]],\n",
        "                                [0.0,                    0.0,                  1.0]])\n",
        "        goal_waypoint = self.plan.poses[self.goal_waypoint_idx].pose.position\n",
        "        goal_waypoint_arr = np.array([goal_waypoint.x, goal_waypoint.y, 1.0]).T\n",
        "        projected = np.dot(np.linalg.inv(robot_frame), goal_waypoint_arr) # use .T TODO\n",
        "        goal_heading = np.arctan2(projected[1], projected[0])\n",
        "        # print(projected, goal_heading)\n",
        "        if not np.isclose(goal_heading, 0.0, atol=0.15):\n",
        "            return Wheels(left_wheel=-np.sign(goal_heading) * self.rotation_speed * (abs(goal_heading) / 1.57),\n",
        "                          right_wheel=np.sign(goal_heading) * self.rotation_speed * (abs(goal_heading) / 1.57))\n",
        "\n",
        "        # perform translation\n",
        "        return Wheels(left_wheel=self.translation_speed, right_wheel=self.translation_speed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMejK44-2M75"
      },
      "source": [
        "We show how this NaiveController is able to get the robot to the goal when given ideal conditions (no noise and perfect world knowledge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxtIrbhUD4Kd"
      },
      "source": [
        "# Execute planned path\n",
        "sim = Simulator('example_apartment.json', load_landmarks=False, odom_sigma_x=0.0, odom_sigma_y=0.0, odom_sigma_h=0.0)\n",
        "controller = NaiveController(response.plan, initial_pose=[1.0, 1.0, 0.0])\n",
        "cmd = Wheels(left_wheel=0.0, right_wheel=0.0)\n",
        "with tqdm(total=125) as pbar:\n",
        "    for _ in range(125): # simulate out 125 seconds\n",
        "        odom, _, _ = sim.step(cmd, None, duration=1.0)\n",
        "        cmd = controller.execute_plan(odom)\n",
        "        pbar.update()\n",
        "\n",
        "sim.visualize(plan=response.plan, show_grid_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfOQW7zqaFs6"
      },
      "source": [
        "But what happens when we stop assuming no noise?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j8_zfIaaC1a"
      },
      "source": [
        "ODOM_NOISE_SIGMA_X = 4e-3\n",
        "ODOM_NOISE_SIGMA_Y = 4e-3\n",
        "ODOM_NOISE_SIGMA_HEADING = 8e-3\n",
        "\n",
        "# Execute planned path\n",
        "sim = Simulator('example_apartment.json', load_landmarks=False,\n",
        "                odom_sigma_x=ODOM_NOISE_SIGMA_X, odom_sigma_y=ODOM_NOISE_SIGMA_Y,\n",
        "                odom_sigma_h=ODOM_NOISE_SIGMA_HEADING)\n",
        "controller = NaiveController(response.plan, initial_pose=[1.0, 1.0, 0.0])\n",
        "cmd = Wheels(left_wheel=0.0, right_wheel=0.0)\n",
        "with tqdm(total=125) as pbar:\n",
        "    for _ in range(125): # simulate out 125 seconds\n",
        "        odom, _, _ = sim.step(cmd, None, duration=1.0)\n",
        "        cmd = controller.execute_plan(odom)\n",
        "        pbar.update()\n",
        "\n",
        "sim.visualize(plan=response.plan, show_grid_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQOxhe-Ubxe9"
      },
      "source": [
        "Also note that we explicitly had to tell the NaiveController that the robot started at the initial position in world frame of (1.0, 1.0). We'll see how the particle filter will enable us to automatically figure out our position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewf7v0dYgB9J"
      },
      "source": [
        "# Particle Filter\n",
        "\n",
        "In this section, we'll implement the particle filter. The cell below provides the template for it. Below it are test and scenarios to help test your particle filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLj-jZsHgn8T"
      },
      "source": [
        "class ParticleFilter:\n",
        "\n",
        "    def __init__(self, sim, num_particles=5000, injected_xy_sigma=0.1, injected_heading_sigma=0.1, range_sigma=0.5, bearing_sigma=0.3):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sim: Simulator\n",
        "            simulation environment\n",
        "        num_particles: int\n",
        "            number of particles to keep in the particle filter\n",
        "        injected_xy_sigma: float\n",
        "            sigma of additional noise to inject in (x, y) components during motion update\n",
        "        injected_heading_sigma: float\n",
        "            sigma of additional noise to inject in the heading component during motion update\n",
        "        range_sigma: float\n",
        "            sigma of noise in landmark range measurement\n",
        "        bearing_sigma: float\n",
        "            sigma of noise in landmark bearing measurement\n",
        "        \"\"\"\n",
        "        self.sim = sim\n",
        "        self.num_particles = num_particles\n",
        "        self.range_sigma = range_sigma\n",
        "        self.bearing_sigma = bearing_sigma\n",
        "        self.injected_xy_sigma = injected_xy_sigma\n",
        "        self.injected_heading_sigma = injected_heading_sigma\n",
        "        self.prev_x = 0.0\n",
        "        self.prev_y = 0.0\n",
        "        self.prev_heading = 0.0\n",
        "        self.particles = []\n",
        "        self.initialize_particles()\n",
        "\n",
        "    def generate_random_particles(self, num_particles):\n",
        "        \"\"\"\n",
        "        Generate particles in random locations within the free space of map\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_particles: int\n",
        "            number of particles to generate\n",
        "        \n",
        "        Return\n",
        "        ----------\n",
        "        Numpy array of shape (N, 3), where N is the number of particles and each\n",
        "            particles has an x (meters), y (meters), and heading (radians) component.\n",
        "            Particles are given with respect to the map frame (i.e. the bottom left\n",
        "            corner of the map)\n",
        "        \"\"\"\n",
        "        particles = []\n",
        "        while len(particles) < num_particles:\n",
        "            particle_x = np.random.uniform(0.15, self.sim.env_manager.map_json['room']['xsize_m'] - 0.15)\n",
        "            particle_y = np.random.uniform(0.15, self.sim.env_manager.map_json['room']['ysize_m'] - 0.15)\n",
        "            if (self.sim.is_free(particle_x, particle_y)):\n",
        "                particle_h = np.random.uniform(-np.pi, np.pi)\n",
        "                particle = (particle_x, particle_y, particle_h)\n",
        "                particles.append(particle)\n",
        "        return np.array(particles)\n",
        "\n",
        "    def initialize_particles(self):\n",
        "        \"\"\"\n",
        "        Initialize the particles in random locations within the free space of the map\n",
        "        \"\"\"\n",
        "        self.particles = self.generate_random_particles(self.num_particles)\n",
        "\n",
        "    def compute_estimate(self):\n",
        "        \"\"\"\n",
        "        Compute the pose estiamte as the average of all particles\n",
        "        \"\"\"\n",
        "        particles = np.array(self.particles)\n",
        "        x_arr = particles[:, 0]\n",
        "        y_arr = particles[:, 1]\n",
        "        h_arr = particles[:, 2] \n",
        "        avg_x = np.average(x_arr)\n",
        "        avg_y = np.average(y_arr)\n",
        "        avg_cos = np.average(np.cos(h_arr))\n",
        "        avg_sin = np.average(np.sin(h_arr))\n",
        "        avg_h = np.arctan2(avg_sin, avg_cos)\n",
        "        return (avg_x, avg_y, avg_h)\n",
        "\n",
        "    def is_confident(self):\n",
        "        \"\"\"\n",
        "        provide the confidence of esitmation by checking if 95% of particles are close\n",
        "        to the average pose\n",
        "        \"\"\"\n",
        "        avg_pose = self.compute_estimate()\n",
        "        count = 0\n",
        "        for particle in self.particles:\n",
        "            diff_x = abs(particle[0] - avg_pose[0])\n",
        "            diff_y = abs(particle[1] - avg_pose[1])\n",
        "            diff_theta = abs(particle[2] - avg_pose[2])\n",
        "            if (diff_theta > np.pi):\n",
        "                diff_theta = 2 * np.pi - diff_theta\n",
        "            if (diff_x ** 2 + diff_y ** 2) ** 0.5 < 1 and diff_theta < 0.5:\n",
        "                count += 1\n",
        "        return count / self.num_particles >= 0.95\n",
        "\n",
        "    def motion_update(self, rel_pose, step_sigma):\n",
        "        \"\"\"Move all particles by the given odometry.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rel_pose: tuple of (float, float, float)\n",
        "            (x, y, theta) represeting the relative pose between the current step (k) and the \n",
        "            previous step (k-1) (T_{k-1}^k)\n",
        "        step_sigma: tuple of (float, float, float)\n",
        "            (sigma_x, sigma_y, sigma_theta) represeting the standard deviation for the noise \n",
        "            in the x, y, theta components of the relative pose. Assume the noise in x, y, theta \n",
        "            are independent\n",
        "\n",
        "        Effect\n",
        "        ----------\n",
        "            Update self.particles so that each particle has moved by the given odometry and some \n",
        "            noise.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Be sure to transform the odometry to each particle's frame. You may find the helper \n",
        "            function \"pose_compose\" helpful\n",
        "        - Don't worry about particles leaving the grid/entering obstacles here. You\n",
        "            will give these particles weights of zero to ensure they aren't sampled.\n",
        "        - self.particles is a Numpy array of shape (N, 3), where N is the number of particles and \n",
        "            each particles has an x (meters), y (meters), and heading (radians) component.\n",
        "        - Use np.random.normal to add noise.\n",
        "        - When calculating sigma to apply noise to the particle's motion update,\n",
        "            include `self.xy_sigma` and `self.heading_sigma` as well as the sigmas\n",
        "            passed in from the simulator.\n",
        "        \"\"\"\n",
        "        # STUDENT TODO\n",
        "        return\n",
        "\n",
        "    def measurement_update(self, measured_marker_list):\n",
        "        \"\"\" Weight the particles by measurements, and resample particles according to their weights\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        measured_marker_list : list of RangeBearingLandmark object\n",
        "            all bearing range measurements (with noise) of markers collected by the robot at this step\n",
        "            each RangeBearingLandmark object has 3 attributes:\n",
        "                RangeBearingLandmark.id : string (type of the marker)\n",
        "                RangeBearingLandmark.range : double (range measurement)\n",
        "                RangeBearingLandmark.bearing : double (bearing measurement)\n",
        "        \n",
        "        Effect\n",
        "        ----------\n",
        "            Weight the particles by measurements, and resample particles according to their weights. Create \n",
        "            a small fraction of random particles and add to self.particles.\n",
        "\n",
        "        Notes\n",
        "        -------\n",
        "        - You can call the \"self.sim.visible_markers_gt(pose)\" to get the bearing range measurements (without noise) \n",
        "            of all visible landmarks at the specified pose (pose is specified by the tuple (x, y, theta)). The \n",
        "            function returns the same type as \"measured_marker_list\".\n",
        "        - Markers are associated with an 'id' that identifies the type of the marker. Multiple\n",
        "            markers can have the same id. If multiple markers with the same id are observed, you need to perform \n",
        "            matching across landmarks\n",
        "        - If there are markers cannot be matched, you can simply set the weight of the corresponding particle to 0\n",
        "        - You can add a small fraction of additional randomly sampled particles with \"generate_random_particles\" \n",
        "            function\n",
        "\n",
        "        \"\"\"\n",
        "        # Student TODO\n",
        "        return\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0V2-A5DdlX4"
      },
      "source": [
        "We have provided a function called `initialize_particles()` to initialize the particles randomly at valid locations in the map.\n",
        "\n",
        "In this cell, you'll visualize what this initialization of particles looks like. You may play with the number of particles by passing in a number to the `num_particles` parameter in the ParticleFilter class constructor. Note that each particle has an orienatation in the map, however, we do not visualize the angle of the particles here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4SvXG-Edk13"
      },
      "source": [
        "sim = Simulator('example_apartment.json')\n",
        "\n",
        "pf = ParticleFilter(sim, num_particles=200)\n",
        "pf.initialize_particles()\n",
        "\n",
        "sim.step(Wheels(), pf.particles)\n",
        "sim.visualize(show_grid_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZgg41Wme0Pz"
      },
      "source": [
        "###Task 2: Motion Update\n",
        "\n",
        "Implement the `motion_update()` method in the ParticleFilter class. Notice that you can compute the pose of current step $T_w^k$ ($w$ represents world frame) from the pose of the previous step $T_w^{k-1}$ and the odometry(relative pose) $T_{k-1}^k$ by\n",
        "\n",
        "$T_w^k=T_w^{k-1} T_{k-1}^k$\n",
        "\n",
        "Below, we provide you a code snippet that enables you to run motion updates with hardcoded motions. Because we have turned off noise, you should expect to see the particles only move forward in a straight line.\n",
        "\n",
        "You may modify the hardcoded commands to see other motions. You may add noise to the simulator (which determine that values for `sigmas` returned from `sim.step()`) to see how the particles move with noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usphx57ze1MZ"
      },
      "source": [
        "sim = Simulator('example_apartment.json', odom_sigma_x=0.0, odom_sigma_y=0.0, odom_sigma_h=0.0)\n",
        "\n",
        "pf = ParticleFilter(sim, num_particles=10, injected_xy_sigma=0.0, injected_heading_sigma=0.0)\n",
        "pf.initialize_particles()\n",
        "\n",
        "odom, sigmas, _ = sim.step(Wheels(left_wheel=1.0, right_wheel=1.0), pf.particles, duration=2.0)\n",
        "pf.motion_update(odom, sigmas)\n",
        "odom, sigmas, _ = sim.step(Wheels(left_wheel=1.0, right_wheel=1.0), pf.particles, duration=2.0)\n",
        "pf.motion_update(odom, sigmas)\n",
        "odom, sigmas, _ = sim.step(Wheels(left_wheel=1.0, right_wheel=1.0), pf.particles, duration=2.0)\n",
        "pf.motion_update(odom, sigmas)\n",
        "odom, sigmas, _ = sim.step(Wheels(left_wheel=1.0, right_wheel=1.0), pf.particles, duration=2.0)\n",
        "pf.motion_update(odom, sigmas)\n",
        "odom, sigmas, _ = sim.step(Wheels(left_wheel=0.0, right_wheel=0.0), pf.particles, duration=1.0)\n",
        "\n",
        "sim.visualize(show_grid_lines=True, duration=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOuYYyFwhA_u"
      },
      "source": [
        "###Task 3: Measurement Update\n",
        "\n",
        "Implement the `measurement_update()` method in the ParticleFilter class. Note that to the probablity density for a bearing range measurement $(b, r)$ with ground truth $(b_{gt}, r_{gt})$ is provided by\n",
        "\n",
        "$P(b,r)\\propto exp({\\frac{(b-b_{gt})^2}{\\sigma_b^2/2}}) * exp({\\frac{(r-r_{gt})^2}{\\sigma_r^2/2}})$\n",
        "\n",
        "Below, we provide you a code snippet that enables you to run measurement updates with hardcoded motions. You should be able to see the particles converge on the ground truth position of the robot as the measurements are used to filter out unlikely particles.\n",
        "\n",
        "You may modify the hardcoded commands to see other motions. You may play with the injected noise (`xy_sigma` and `heading_sigma` parameters below) in the motion update. You should not change the odometry and measurement noise given in each example below (we would not be able to change it with a real robot because it would be determined by hardware/sensors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVKexygnh8oi"
      },
      "source": [
        "ODOM_NOISE_SIGMA_X = 4e-4\n",
        "ODOM_NOISE_SIGMA_Y = 4e-4\n",
        "ODOM_NOISE_SIGMA_HEADING = 8e-4\n",
        "MEASURE_NOISE_SIGMA_RANGE = 0.5\n",
        "MEASURE_NOISE_SIGMA_BEARING = 0.3\n",
        "\n",
        "sim = Simulator('example_apartment.json', odom_sigma_x=ODOM_NOISE_SIGMA_X, odom_sigma_y=ODOM_NOISE_SIGMA_Y,\n",
        "                odom_sigma_h=ODOM_NOISE_SIGMA_HEADING, marker_sigma_range=MEASURE_NOISE_SIGMA_RANGE,\n",
        "                marker_sigma_bearing=MEASURE_NOISE_SIGMA_BEARING, initial_pose=[5.5, 3.3, 0.0])\n",
        "\n",
        "pf = ParticleFilter(sim, num_particles=500, injected_xy_sigma=0.1, injected_heading_sigma=0.1,\n",
        "                    range_sigma=MEASURE_NOISE_SIGMA_RANGE, bearing_sigma=MEASURE_NOISE_SIGMA_BEARING)\n",
        "pf.initialize_particles()\n",
        "\n",
        "with tqdm(total=40) as pbar:\n",
        "    for _ in range(40): # simulate out 20 seconds\n",
        "        odom, sigmas, measured_markers = sim.step(Wheels(left_wheel=2.5, right_wheel=1.0), pf.particles, duration=0.5)\n",
        "        pf.motion_update(odom, sigmas)\n",
        "        pf.measurement_update(measured_markers)\n",
        "        pbar.update()\n",
        "\n",
        "sim.visualize(show_grid_lines=True, duration=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80P7RGa3h9Z4"
      },
      "source": [
        "The scenario here is similar to the circular motion shown in the cell above. Here we see the robot moving left from the right handside of the room."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thKZWSQ8eUpG"
      },
      "source": [
        "ODOM_NOISE_SIGMA_X = 4e-4\n",
        "ODOM_NOISE_SIGMA_Y = 4e-4\n",
        "ODOM_NOISE_SIGMA_HEADING = 8e-4\n",
        "MEASURE_NOISE_SIGMA_RANGE = 0.5\n",
        "MEASURE_NOISE_SIGMA_BEARING = 0.3\n",
        "\n",
        "sim = Simulator('example_apartment.json', odom_sigma_x=ODOM_NOISE_SIGMA_X, odom_sigma_y=ODOM_NOISE_SIGMA_Y,\n",
        "                odom_sigma_h=ODOM_NOISE_SIGMA_HEADING, marker_sigma_range=MEASURE_NOISE_SIGMA_RANGE,\n",
        "                marker_sigma_bearing=MEASURE_NOISE_SIGMA_BEARING, initial_pose=[8.0, 1.3, np.pi])\n",
        "\n",
        "pf = ParticleFilter(sim, num_particles=500, injected_xy_sigma=0.1, injected_heading_sigma=0.1,\n",
        "                    range_sigma=MEASURE_NOISE_SIGMA_RANGE, bearing_sigma=MEASURE_NOISE_SIGMA_BEARING)\n",
        "pf.initialize_particles()\n",
        "\n",
        "with tqdm(total=50) as pbar:\n",
        "    for _ in range(50): # simulate out 50 seconds\n",
        "        odom, sigmas, measured_markers = sim.step(Wheels(left_wheel=2.0, right_wheel=2.0), pf.particles, duration=1.0)\n",
        "        pf.motion_update(odom, sigmas)\n",
        "        pf.measurement_update(measured_markers)\n",
        "        pbar.update()\n",
        "\n",
        "sim.visualize(show_grid_lines=True, duration=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYECv_CMDtgu"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ_-zvj6n4Nd"
      },
      "source": [
        "### Task 4: Path Tracking with an Informed Controller\n",
        "\n",
        "Implement the `execute_plan` method in the InformedController class below. You may refer to the NaiveController class implemented provided in the \"Planning\" section. Note that NaiveController's `execute_plan()` method is given relative odometry updates (dx, dy, dtheta), whereas the InformedController's is given a pose estimate in the world frame (x_w, y_w, theta_w) that is estimated by the particle filter. Additionally, it is given a boolean `is_confident`, which tells you whether at least 95% of the particles agreed with the mean pose of all particles. You should use the `is_confident` boolean to change the behavior of your robot. If you are not confident about where you are, use a hard-coded convergence method (see two examples in Task 3 above) until the particle filter has seen enough measurements to give more confident estimates. Once confident, start execution of the path. If you lose confidence while tracking the path, revert to the search mode once more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-sYdWb9cJuX"
      },
      "source": [
        "class InformedController:\n",
        "\n",
        "    def __init__(self, plan, max_rotational_speed=1.0, max_translation_speed=2.5):\n",
        "        self.plan = plan\n",
        "        self.rotation_speed = max_rotational_speed\n",
        "        self.translation_speed = max_translation_speed\n",
        "\n",
        "    def _within_tolerance_of_point(self, x, y, waypoint, tolerance_m):\n",
        "        def distance(x1, y1, x2, y2):\n",
        "            return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
        "\n",
        "        return distance(waypoint.x, waypoint.y, x, y) <= tolerance_m\n",
        "\n",
        "    def execute_plan(self, pose, is_confident):\n",
        "        \"\"\"\n",
        "        compute the wheel commands for a small time step to follwo the planned trajectory\n",
        "\n",
        "        Parameters\n",
        "        --------\n",
        "        pose : (float, float, float)\n",
        "            estimate of current pose in (x, y, theta)\n",
        "        is_confident : bool\n",
        "            if the current pose estiamte is confident\n",
        "        \"\"\"\n",
        "        # STUDENT TODO\n",
        "        return Wheels(left_wheel=0.0, right_wheel=0.0)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wx-j8AIZE7o"
      },
      "source": [
        "Below we provide a complete pipeline that robot uses to plan the path with RRT, estimate its pose with Monte Carlo Localization, and path tracking. You should see your robot able to execute the path despite\n",
        "1. being given **no** information on where the robot started (the kidnapped robot problem)\n",
        "2. significant noise in the odometry and measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFSkxV4b0Cxh"
      },
      "source": [
        "# create the simulator for the \"example_apartment\" map\n",
        "sim = Simulator('example_apartment.json')\n",
        "\n",
        "# create RRTPlanner service request/response objects\n",
        "request = GetPlan.Request()\n",
        "start_point = Point(x=1.0, y=1.0)\n",
        "goal_point = Point(x=6.2, y=3.0)\n",
        "request.start = PoseStamped(pose=Pose(position=start_point))\n",
        "request.goal = PoseStamped(pose=Pose(position=goal_point))\n",
        "request.tolerance = 0.1 # meters\n",
        "response = GetPlan.Response()\n",
        "\n",
        "# load obstacles and trigger planning\n",
        "duckie_rrt_planner = RRTPlanner()\n",
        "duckie_rrt_planner.obstacles = sim.get_obstacles()\n",
        "response = duckie_rrt_planner.plan_callback(request, response)\n",
        "\n",
        "# initialize particle filter\n",
        "pf = ParticleFilter(sim, num_particles=1000)\n",
        "pf.initialize_particles()\n",
        "\n",
        "# execute planned path\n",
        "controller = InformedController(response.plan)\n",
        "cmd = Wheels(left_wheel=0.0, right_wheel=0.0)\n",
        "with tqdm(total=100) as pbar:\n",
        "    for t in range(100): # simulate out 50 seconds\n",
        "        odom, sigmas, measured_markers = sim.step(cmd, pf.particles, duration=0.5)\n",
        "        pf.motion_update(odom, sigmas)\n",
        "        pf.measurement_update(measured_markers)\n",
        "        cmd = controller.execute_plan(pf.compute_estimate(), pf.is_confident())\n",
        "        pbar.update()\n",
        "\n",
        "# visualize the robot, plan, and particles\n",
        "sim.visualize(plan=response.plan, show_grid_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cPgPCWSzHEz"
      },
      "source": [
        "**Take a video of the animation generated by the cell above. Submit this video as a project deliverable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0gKt2Jw8xs"
      },
      "source": [
        "# Reflection & Submission\n",
        "\n",
        "Complete the reflection questions in the proj4_report_template.pptx, which you can find in the files tab on Canvas. Failure to follow the format will be penalized. Save the report as a .pdf and rename it to \"FIRSTNAME_LASTNAME_reflection.pdf\".\n",
        "\n",
        "### Rubric\n",
        "\n",
        " * 30 points: Particle Filter Implementation\n",
        "   - 10 points: Motion Update\n",
        "   - 20 points: Measurement Update\n",
        " * 20 points: Informed Controller\n",
        " * 30 points: Demo Video\n",
        " * 20 points: Reflection\n",
        "\n",
        "### Submission Details\n",
        "\n",
        "Deliverables are a zip file named \"FIRSTNAME_LASTNAME_project4.zip\" with the following files:\n",
        "\n",
        " * project4.py - exported from Google Colab (File -> Download .py)\n",
        " * FIRSTNAME_LASTNAME_reflection.pdf - exported from the given powerpoint\n",
        " * FIRSTNAME_LASTNAME_video.{mov, mp4} - No longer than 30 seconds\n",
        "\n",
        "This is an individual assignment, everyone should submit their own files.\n",
        "\n"
      ]
    }
  ]
}